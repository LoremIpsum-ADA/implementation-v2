{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJWgAmhTMzXK",
        "outputId": "d907d9d4-a8fe-4ae7-8106-6598fe265db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19sq2HQipIoIcJvsID_M8iB84Ap-2lJje\n",
            "From (redirected): https://drive.google.com/uc?id=19sq2HQipIoIcJvsID_M8iB84Ap-2lJje&confirm=t&uuid=b7a624b7-6eee-4898-9704-99acd506c7d7\n",
            "To: /content/data.zip\n",
            "100%|██████████| 9.01G/9.01G [01:13<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Data extracted\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.4/338.4 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m158.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.3/233.3 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✓ Config set: endangered_asia_disaster_analysis_dml\n",
            "✓ Data loading functions defined\n",
            "✓ Species list: 13 species\n",
            "✓ Grid created: 23,400 cells\n",
            "Loaded 506,589 records\n",
            "After filtering: 506,589 records\n",
            "✓ Balanced panel: 7,605,000 rows, occupancy rate: 0.16%\n",
            "Loaded 766 disaster events\n",
            "✓ Treatment panel: 592 treated cells\n",
            "✓ Analysis panel: 7,605,000 obs, 23,400 cells, treatment rate: 0.12%\n",
            "✓ Double ML functions defined\n",
            "\n",
            "================================================================================\n",
            "RUNNING DOUBLE ML ANALYSIS\n",
            "================================================================================\n",
            "Feature matrix: 7,605,000 rows × 117 features\n",
            "Running Double ML (5-fold cross-fitting), n=7,605,000, treatment rate=0.12%\n",
            "  Fold 1/5... ✓\n",
            "  Fold 2/5... "
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# COMPLETE DOUBLE ML ANALYSIS - GOOGLE COLAB\n",
        "# ============================================================================\n",
        "\n",
        "# STEP 0: DOWNLOAD DATA\n",
        "!pip install -q gdown\n",
        "import gdown\n",
        "import os\n",
        "FILE_ID = \"19sq2HQipIoIcJvsID_M8iB84Ap-2lJje\"\n",
        "output_zip = \"/content/data.zip\"\n",
        "url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
        "gdown.download(url, output_zip, quiet=False)\n",
        "!unzip -q \"{output_zip}\" -d /content/\n",
        "print(\"✓ Data extracted\")\n",
        "\n",
        "# STEP 1: INSTALL PACKAGES\n",
        "!pip install -q python-dotenv scikit-learn linearmodels geopandas matplotlib seaborn scipy\n",
        "\n",
        "# STEP 2: IMPORTS & CONFIGURATION\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box, Point\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy import stats\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "BASE_PATH = Path('/content')\n",
        "DATA_DIR = BASE_PATH / 'data'\n",
        "\n",
        "CONFIG = {\n",
        "    'study_name': 'endangered_asia_disaster_analysis_dml',\n",
        "    'data_dir': DATA_DIR,\n",
        "    'results_dir': BASE_PATH / 'results',\n",
        "    'notebook_generated_on': datetime.datetime.utcnow().isoformat() + 'Z',\n",
        "    'grid_resolution_deg': 0.5,\n",
        "    'region_bbox': [60.0, -10.0, 150.0, 55.0],\n",
        "    'time_unit': 'year',\n",
        "    'time_range': (2000, 2024),\n",
        "    'taxa': ['Aves', 'Mammalia', 'Reptilia', 'Amphibia'],\n",
        "    'target_status': ['CR', 'EN', 'VU'],\n",
        "    'min_occurrences': 50,\n",
        "    'disaster_types': ['wildfire', 'flood', 'cyclone', 'earthquake'],\n",
        "    'event_window': (-3, 5),\n",
        "    'ml_model': 'random_forest',\n",
        "    'n_folds': 5,\n",
        "    'save_intermediate': True,\n",
        "    'figure_format': 'png',\n",
        "    'figure_dpi': 300,\n",
        "}\n",
        "\n",
        "for dir_path in [CONFIG['data_dir'], CONFIG['results_dir']]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "    (dir_path / 'raw').mkdir(exist_ok=True)\n",
        "    (dir_path / 'processed').mkdir(exist_ok=True)\n",
        "\n",
        "with open(CONFIG['results_dir'] / 'config.json', 'w') as f:\n",
        "    json.dump({k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()}, f, indent=2)\n",
        "\n",
        "print(f\"✓ Config set: {CONFIG['study_name']}\")\n",
        "\n",
        "# STEP 3: DATA LOADING FUNCTIONS\n",
        "def create_analysis_grid(bbox, resolution):\n",
        "    min_lon, min_lat, max_lon, max_lat = bbox\n",
        "    lons = np.arange(min_lon, max_lon, resolution)\n",
        "    lats = np.arange(min_lat, max_lat, resolution)\n",
        "    grid_polys = []\n",
        "    grid_ids = []\n",
        "    grid_centers = []\n",
        "    for i, lon in enumerate(lons):\n",
        "        for j, lat in enumerate(lats):\n",
        "            poly = box(lon, lat, lon + resolution, lat + resolution)\n",
        "            grid_polys.append(poly)\n",
        "            grid_ids.append(f'g_{i}_{j}')\n",
        "            grid_centers.append((lon + resolution/2, lat + resolution/2))\n",
        "    grid_gdf = gpd.GeoDataFrame({\n",
        "        'grid_id': grid_ids,\n",
        "        'lon_center': [c[0] for c in grid_centers],\n",
        "        'lat_center': [c[1] for c in grid_centers],\n",
        "        'geometry': grid_polys\n",
        "    }, crs='EPSG:4326')\n",
        "    grid_gdf['area_km2'] = grid_gdf.to_crs('EPSG:3857').geometry.area / 1e6\n",
        "    return grid_gdf\n",
        "\n",
        "def map_points_to_grid(points_gdf, grid_gdf):\n",
        "    pts = points_gdf.to_crs(grid_gdf.crs)\n",
        "    joined = gpd.sjoin(pts, grid_gdf[['grid_id', 'geometry']], how='left', predicate='within')\n",
        "    joined = joined.dropna(subset=['grid_id'])\n",
        "    return joined\n",
        "\n",
        "def load_occurrence_data(filepath, species_list, year_min, year_max, bbox):\n",
        "    if not filepath.exists():\n",
        "        print(f\"⚠ File not found: {filepath}\")\n",
        "        return None\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, sep='\\t', encoding='utf-8', engine='python', quoting=3)\n",
        "        if len(df.columns) < 10:\n",
        "            df = pd.read_csv(filepath, sep=',', encoding='utf-8', low_memory=False)\n",
        "    except:\n",
        "        df = pd.read_csv(filepath, sep=',', encoding='utf-8', low_memory=False)\n",
        "    print(f\"Loaded {len(df):,} records\")\n",
        "    if 'species' in df.columns:\n",
        "        df = df[df['species'].isin(species_list)]\n",
        "    if 'year' in df.columns:\n",
        "        df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
        "        df = df[(df['year'] >= year_min) & (df['year'] <= year_max)]\n",
        "    if 'decimalLatitude' in df.columns and 'decimalLongitude' in df.columns:\n",
        "        df = df[(df['decimalLongitude'] >= bbox[0]) & (df['decimalLongitude'] <= bbox[2]) &\n",
        "                (df['decimalLatitude'] >= bbox[1]) & (df['decimalLatitude'] <= bbox[3])]\n",
        "        df = df.dropna(subset=['decimalLatitude', 'decimalLongitude'])\n",
        "    print(f\"After filtering: {len(df):,} records\")\n",
        "    return df\n",
        "\n",
        "def load_disaster_data(filepath, disaster_types, year_min, year_max, bbox):\n",
        "    if not filepath.exists():\n",
        "        print(f\"⚠ File not found: {filepath}\")\n",
        "        return None\n",
        "    df = pd.read_csv(filepath, encoding='utf-8-sig', low_memory=False)\n",
        "    if 'Start Year' in df.columns:\n",
        "        df = df[(df['Start Year'] >= year_min) & (df['Start Year'] <= year_max)]\n",
        "        df = df.rename(columns={'Start Year': 'year'})\n",
        "    if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
        "        df = df.dropna(subset=['Latitude', 'Longitude'])\n",
        "        df = df[(df['Longitude'] >= bbox[0]) & (df['Longitude'] <= bbox[2]) &\n",
        "                (df['Latitude'] >= bbox[1]) & (df['Latitude'] <= bbox[3])]\n",
        "        df = df.rename(columns={'Latitude': 'latitude', 'Longitude': 'longitude'})\n",
        "    print(f\"Loaded {len(df):,} disaster events\")\n",
        "    return df\n",
        "\n",
        "print(\"✓ Data loading functions defined\")\n",
        "\n",
        "# STEP 4: LOAD SPECIES LIST\n",
        "species_data = {\n",
        "    'taxon': ['Aves', 'Aves', 'Aves', 'Aves', 'Mammalia', 'Mammalia', 'Mammalia', 'Mammalia', 'Reptilia', 'Reptilia', 'Reptilia', 'Amphibia', 'Amphibia'],\n",
        "    'scientificName': ['Lophura edwardsi', 'Arborophila davidi', 'Turdoides striata', 'Carpococcyx renauldi',\n",
        "                       'Panthera tigris', 'Elephas maximus', 'Rhinoceros sondaicus', 'Pongo abelii',\n",
        "                       'Crocodylus siamensis', 'Chelonia mydas', 'Cuora trifasciata',\n",
        "                       'Ansonia latidisca', 'Rhacophorus catamitus'],\n",
        "    'status': ['CR', 'EN', 'VU', 'EN', 'EN', 'EN', 'CR', 'CR', 'CR', 'EN', 'CR', 'EN', 'VU']\n",
        "}\n",
        "species_df = pd.DataFrame(species_data)\n",
        "species_df.to_csv(CONFIG['data_dir'] / 'processed' / 'target_species.csv', index=False)\n",
        "print(f\"✓ Species list: {len(species_df)} species\")\n",
        "\n",
        "# STEP 5: CREATE SPATIAL GRID\n",
        "grid_gdf = create_analysis_grid(CONFIG['region_bbox'], CONFIG['grid_resolution_deg'])\n",
        "print(f\"✓ Grid created: {len(grid_gdf):,} cells\")\n",
        "grid_path = CONFIG['data_dir'] / 'processed' / 'analysis_grid.gpkg'\n",
        "grid_gdf.to_file(grid_path, driver='GPKG')\n",
        "\n",
        "# STEP 6: LOAD OCCURRENCE DATA\n",
        "gbif_file = CONFIG['data_dir'] / 'raw' / 'gbif' / '0019239-251025141854904.csv'\n",
        "if gbif_file.exists():\n",
        "    occurrence_df = load_occurrence_data(gbif_file, species_df['scientificName'].tolist(),\n",
        "                                         CONFIG['time_range'][0], CONFIG['time_range'][1], CONFIG['region_bbox'])\n",
        "    if occurrence_df is not None and len(occurrence_df) > 0:\n",
        "        occ_gdf = gpd.GeoDataFrame(occurrence_df,\n",
        "                                   geometry=gpd.points_from_xy(occurrence_df['decimalLongitude'],\n",
        "                                                               occurrence_df['decimalLatitude']), crs='EPSG:4326')\n",
        "        occ_joined = map_points_to_grid(occ_gdf, grid_gdf)\n",
        "        years_list = list(range(CONFIG['time_range'][0], CONFIG['time_range'][1] + 1))\n",
        "        species_list = species_df['scientificName'].tolist()\n",
        "        grid_ids = grid_gdf['grid_id'].unique()\n",
        "        observed = occ_joined.groupby(['grid_id', 'year', 'species']).size().reset_index(name='n_occurrences')\n",
        "        observed['occupancy'] = 1\n",
        "        all_combos = pd.DataFrame(list(itertools.product(grid_ids, years_list, species_list)),\n",
        "                                  columns=['grid_id', 'year', 'species'])\n",
        "        occurrence_panel = all_combos.merge(observed, on=['grid_id', 'year', 'species'], how='left')\n",
        "        occurrence_panel['n_occurrences'] = occurrence_panel['n_occurrences'].fillna(0)\n",
        "        occurrence_panel['occupancy'] = occurrence_panel['occupancy'].fillna(0)\n",
        "        print(f\"✓ Balanced panel: {len(occurrence_panel):,} rows, occupancy rate: {occurrence_panel['occupancy'].mean()*100:.2f}%\")\n",
        "        occurrence_panel.to_csv(CONFIG['data_dir'] / 'processed' / 'occurrence_panel.csv', index=False)\n",
        "    else:\n",
        "        occurrence_panel = None\n",
        "else:\n",
        "    print(f\"⚠ GBIF file not found\")\n",
        "    occurrence_panel = None\n",
        "\n",
        "# STEP 7: LOAD DISASTER DATA\n",
        "emdat_file = CONFIG['data_dir'] / 'raw' / 'emdat_asia_2000_2024.csv'\n",
        "if emdat_file.exists():\n",
        "    emdat_df = load_disaster_data(emdat_file, CONFIG['disaster_types'], CONFIG['time_range'][0],\n",
        "                                   CONFIG['time_range'][1], CONFIG['region_bbox'])\n",
        "    if emdat_df is not None:\n",
        "        disaster_gdf = gpd.GeoDataFrame(emdat_df, geometry=gpd.points_from_xy(emdat_df['longitude'],\n",
        "                                                                                emdat_df['latitude']), crs='EPSG:4326')\n",
        "        disaster_joined = map_points_to_grid(disaster_gdf, grid_gdf)\n",
        "        treatment_df = disaster_joined.groupby(['grid_id', 'year']).size().reset_index(name='n_disasters')\n",
        "        treatment_df['treated'] = 1\n",
        "        first_treatment = treatment_df.groupby('grid_id')['year'].min().reset_index()\n",
        "        first_treatment.columns = ['grid_id', 'first_treatment_year']\n",
        "        treatment_df = treatment_df.merge(first_treatment, on='grid_id', how='left')\n",
        "        print(f\"✓ Treatment panel: {treatment_df['grid_id'].nunique():,} treated cells\")\n",
        "        treatment_df.to_csv(CONFIG['data_dir'] / 'processed' / 'treatment_panel.csv', index=False)\n",
        "    else:\n",
        "        treatment_df = None\n",
        "else:\n",
        "    print(f\"⚠ EM-DAT file not found\")\n",
        "    treatment_df = None\n",
        "\n",
        "# STEP 8: CREATE ANALYSIS PANEL\n",
        "if occurrence_panel is not None and treatment_df is not None:\n",
        "    panel_df = occurrence_panel.merge(treatment_df[['grid_id', 'year', 'treated', 'first_treatment_year']],\n",
        "                                     on=['grid_id', 'year'], how='left')\n",
        "    panel_df['treated'] = panel_df['treated'].fillna(0)\n",
        "    panel_df = panel_df.merge(grid_gdf[['grid_id', 'lon_center', 'lat_center', 'area_km2']], on='grid_id', how='left')\n",
        "    print(f\"✓ Analysis panel: {len(panel_df):,} obs, {panel_df['grid_id'].nunique():,} cells, treatment rate: {panel_df['treated'].mean()*100:.2f}%\")\n",
        "    panel_df.to_csv(CONFIG['data_dir'] / 'processed' / 'analysis_panel.csv', index=False)\n",
        "else:\n",
        "    panel_df = None\n",
        "\n",
        "# STEP 9: DOUBLE ML FUNCTIONS\n",
        "def get_ml_model(model_type, task='regression'):\n",
        "    if task == 'regression':\n",
        "        if model_type == 'random_forest':\n",
        "            return RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=20, n_jobs=-1, random_state=42)\n",
        "        elif model_type == 'gradient_boosting':\n",
        "            return GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
        "        else:\n",
        "            return LassoCV(cv=5, random_state=42)\n",
        "    else:\n",
        "        return RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=20, n_jobs=-1, random_state=42)\n",
        "\n",
        "def prepare_dml_features(panel_df):\n",
        "    features = []\n",
        "    feature_names = []\n",
        "    if 'lon_center' in panel_df.columns:\n",
        "        features.append(panel_df['lon_center'].values.reshape(-1, 1))\n",
        "        feature_names.append('lon_center')\n",
        "    if 'lat_center' in panel_df.columns:\n",
        "        features.append(panel_df['lat_center'].values.reshape(-1, 1))\n",
        "        feature_names.append('lat_center')\n",
        "    if 'year' in panel_df.columns:\n",
        "        features.append(panel_df['year'].values.reshape(-1, 1))\n",
        "        feature_names.append('year')\n",
        "    if 'area_km2' in panel_df.columns:\n",
        "        features.append(panel_df['area_km2'].values.reshape(-1, 1))\n",
        "        feature_names.append('area_km2')\n",
        "    if 'species' in panel_df.columns:\n",
        "        species_dummies = pd.get_dummies(panel_df['species'], prefix='sp')\n",
        "        features.append(species_dummies.values)\n",
        "        feature_names.extend(species_dummies.columns.tolist())\n",
        "    if 'grid_id' in panel_df.columns:\n",
        "        grid_hash = panel_df['grid_id'].apply(lambda x: hash(x) % 100)\n",
        "        grid_dummies = pd.get_dummies(grid_hash, prefix='grid')\n",
        "        features.append(grid_dummies.values)\n",
        "        feature_names.extend(grid_dummies.columns.tolist())\n",
        "    X = np.hstack(features)\n",
        "    print(f\"Feature matrix: {X.shape[0]:,} rows × {X.shape[1]} features\")\n",
        "    return X, feature_names\n",
        "\n",
        "def double_ml_ate(Y, D, X, n_folds=5, ml_model='random_forest', seed=42):\n",
        "    np.random.seed(seed)\n",
        "    Y = np.array(Y)\n",
        "    D = np.array(D)\n",
        "    X = np.array(X) if not isinstance(X, np.ndarray) else X\n",
        "    n = len(Y)\n",
        "    Y_res = np.zeros(n)\n",
        "    D_res = np.zeros(n)\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
        "    print(f\"Running Double ML ({n_folds}-fold cross-fitting), n={n:,}, treatment rate={D.mean()*100:.2f}%\")\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "        print(f\"  Fold {fold_idx + 1}/{n_folds}...\", end=\" \")\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
        "        D_train, D_test = D[train_idx], D[test_idx]\n",
        "        y_model = get_ml_model(ml_model, task='regression')\n",
        "        y_model.fit(X_train, Y_train)\n",
        "        Y_pred = y_model.predict(X_test)\n",
        "        Y_res[test_idx] = Y_test - Y_pred\n",
        "        d_model = get_ml_model(ml_model, task='classification')\n",
        "        d_model.fit(X_train, D_train)\n",
        "        if hasattr(d_model, 'predict_proba'):\n",
        "            D_pred = d_model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            D_pred = d_model.predict(X_test)\n",
        "        D_res[test_idx] = D_test - D_pred\n",
        "        print(\"✓\")\n",
        "    numerator = np.mean(Y_res * D_res)\n",
        "    denominator = np.mean(D_res ** 2)\n",
        "    if denominator < 1e-10:\n",
        "        print(\"⚠ Very small treatment variation\")\n",
        "        return None\n",
        "    ate = numerator / denominator\n",
        "    psi = (Y_res - ate * D_res) * D_res / denominator\n",
        "    se = np.std(psi) / np.sqrt(n)\n",
        "    ci_lower = ate - 1.96 * se\n",
        "    ci_upper = ate + 1.96 * se\n",
        "    pvalue = 2 * (1 - stats.norm.cdf(abs(ate / se)))\n",
        "    print(f\"\\nDOUBLE ML RESULTS:\")\n",
        "    print(f\"ATE: {ate:.6f}, SE: {se:.6f}, 95% CI: [{ci_lower:.6f}, {ci_upper:.6f}], p-value: {pvalue:.6f}\")\n",
        "    return {'ate': ate, 'se': se, 'ci_lower': ci_lower, 'ci_upper': ci_upper, 'pvalue': pvalue, 'residuals': (Y_res, D_res)}\n",
        "\n",
        "def estimate_cate_by_group(Y, D, X, group_var, n_folds=5, ml_model='random_forest'):\n",
        "    unique_groups = np.unique(group_var)\n",
        "    results = {}\n",
        "    print(f\"\\nEstimating CATE for {len(unique_groups)} groups...\")\n",
        "    for group in unique_groups:\n",
        "        print(f\"\\n--- Group: {group} ---\")\n",
        "        mask = (group_var == group)\n",
        "        if mask.sum() < 100:\n",
        "            print(f\"Skipping (n={mask.sum()})\")\n",
        "            continue\n",
        "        try:\n",
        "            result = double_ml_ate(Y[mask], D[mask], X[mask], n_folds=n_folds, ml_model=ml_model)\n",
        "            if result is not None:\n",
        "                results[group] = result\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "    return results\n",
        "\n",
        "def double_ml_event_study(panel_df, outcome_var, entity_var='grid_id', time_var='year',\n",
        "                          treatment_time_var='first_treatment_year', window=(-3, 5), ml_model='random_forest'):\n",
        "    panel = panel_df.copy()\n",
        "    panel['event_time'] = panel[time_var] - panel[treatment_time_var]\n",
        "    X, feature_names = prepare_dml_features(panel)\n",
        "    Y = panel[outcome_var].values\n",
        "    results = []\n",
        "    print(f\"\\nDouble ML Event Study: {window[0]} to {window[1]} years\")\n",
        "    for t in range(window[0], window[1] + 1):\n",
        "        if t == -1:\n",
        "            results.append({'event_time': t, 'ate': 0, 'se': 0, 'ci_lower': 0, 'ci_upper': 0, 'pvalue': 1.0})\n",
        "            continue\n",
        "        print(f\"\\nEvent time t = {t}...\")\n",
        "        D = (panel['event_time'] == t).astype(int).values\n",
        "        if D.sum() < 50:\n",
        "            print(f\"  Skipping (n={D.sum()})\")\n",
        "            continue\n",
        "        try:\n",
        "            result = double_ml_ate(Y, D, X, ml_model=ml_model)\n",
        "            if result is not None:\n",
        "                results.append({'event_time': t, 'ate': result['ate'], 'se': result['se'],\n",
        "                               'ci_lower': result['ci_lower'], 'ci_upper': result['ci_upper'], 'pvalue': result['pvalue']})\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def plot_dml_results(dml_result, title='Double ML Results', save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ate = dml_result['ate']\n",
        "    ci_lower = dml_result['ci_lower']\n",
        "    ci_upper = dml_result['ci_upper']\n",
        "    ax.scatter([0], [ate], s=200, color='darkblue', zorder=3)\n",
        "    ax.errorbar([0], [ate], yerr=[[ate - ci_lower], [ci_upper - ate]],\n",
        "                fmt='none', ecolor='darkblue', capsize=10, capthick=2, linewidth=2)\n",
        "    ax.axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    ax.set_xlim(-0.5, 0.5)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_ylabel('Treatment Effect', fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.grid(alpha=0.3, axis='y')\n",
        "    ax.text(0, ate + (ci_upper - ci_lower) * 0.5,\n",
        "            f'ATE = {ate:.4f}\\n95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\\np = {dml_result[\"pvalue\"]:.4f}',\n",
        "            ha='center', va='bottom', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_dml_event_study(event_study_df, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.plot(event_study_df['event_time'], event_study_df['ate'], 'o-', color='darkblue', linewidth=2, markersize=8)\n",
        "    ax.fill_between(event_study_df['event_time'], event_study_df['ci_lower'], event_study_df['ci_upper'],\n",
        "                    alpha=0.2, color='darkblue')\n",
        "    ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
        "    ax.axvline(-0.5, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
        "    ax.set_xlabel('Years Relative to Disaster', fontsize=12)\n",
        "    ax.set_ylabel('Treatment Effect', fontsize=12)\n",
        "    ax.set_title('Double ML Event Study', fontsize=14, fontweight='bold')\n",
        "    ax.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_cate_forest(cate_results, save_path=None):\n",
        "    groups = list(cate_results.keys())\n",
        "    ates = [r['ate'] for r in cate_results.values()]\n",
        "    ci_lowers = [r['ci_lower'] for r in cate_results.values()]\n",
        "    ci_uppers = [r['ci_upper'] for r in cate_results.values()]\n",
        "    fig, ax = plt.subplots(figsize=(10, max(6, len(groups) * 0.4)))\n",
        "    y_pos = range(len(groups))\n",
        "    ax.errorbar(ates, y_pos, xerr=[[ate - ci_l for ate, ci_l in zip(ates, ci_lowers)],\n",
        "                                     [ci_u - ate for ate, ci_u in zip(ates, ci_uppers)]],\n",
        "                fmt='o', capsize=5, capthick=2, markersize=8, color='darkgreen')\n",
        "    ax.axvline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(groups)\n",
        "    ax.set_xlabel('Treatment Effect (CATE)', fontsize=12)\n",
        "    ax.set_title('Heterogeneous Effects by Group', fontsize=14, fontweight='bold')\n",
        "    ax.grid(alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"✓ Double ML functions defined\")\n",
        "\n",
        "# STEP 10: RUN DOUBLE ML ANALYSIS\n",
        "if panel_df is not None:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RUNNING DOUBLE ML ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    X, feature_names = prepare_dml_features(panel_df)\n",
        "    Y = panel_df['occupancy'].values\n",
        "    D = panel_df['treated'].values\n",
        "    dml_main = double_ml_ate(Y, D, X, n_folds=CONFIG['n_folds'], ml_model=CONFIG['ml_model'])\n",
        "    if dml_main is not None:\n",
        "        pd.DataFrame([dml_main]).to_csv(CONFIG['results_dir'] / 'dml_main_results.csv', index=False)\n",
        "        plot_dml_results(dml_main, title='Double ML: Disaster Impact on Species',\n",
        "                        save_path=CONFIG['results_dir'] / 'dml_main_effect.png')\n",
        "    event_study_df = double_ml_event_study(panel_df, outcome_var='occupancy', window=CONFIG['event_window'],\n",
        "                                           ml_model=CONFIG['ml_model'])\n",
        "    if len(event_study_df) > 0:\n",
        "        event_study_df.to_csv(CONFIG['results_dir'] / 'dml_event_study.csv', index=False)\n",
        "        plot_dml_event_study(event_study_df, save_path=CONFIG['results_dir'] / 'dml_event_study.png')\n",
        "    if 'species' in panel_df.columns:\n",
        "        cate_species = estimate_cate_by_group(Y, D, X, group_var=panel_df['species'].values, ml_model=CONFIG['ml_model'])\n",
        "        if len(cate_species) > 0:\n",
        "            cate_df = pd.DataFrame(cate_species).T\n",
        "            cate_df.index.name = 'species'\n",
        "            cate_df.to_csv(CONFIG['results_dir'] / 'dml_cate_species.csv')\n",
        "            plot_cate_forest(cate_species, save_path=CONFIG['results_dir'] / 'dml_cate_species.png')\n",
        "    print(\"\\n✓ DOUBLE ML ANALYSIS COMPLETE!\")\n",
        "    print(f\"Results saved to: {CONFIG['results_dir']}\")\n",
        "else:\n",
        "    print(\"⚠ Cannot run analysis - panel_df not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wd2DEBiNM4a6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}