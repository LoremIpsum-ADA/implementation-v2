{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa79cb0f",
   "metadata": {},
   "source": [
    "# Causal analysis pipeline — Endangered species in Asia\n",
    "\n",
    "**Notebook purpose:** Complete pipeline (data ingestion → preprocessing → causal analysis → robustness) for endangered species in Asia with natural disasters as treatment.\n",
    "\n",
    "**Geographic scope:** Asia bounding box [60°E, -10°S] to [150°E, 55°N]\n",
    "\n",
    "**Temporal scope:** 2000–2024 (annual aggregation)\n",
    "\n",
    "**Spatial resolution:** 0.1° grid (~11 km at equator)\n",
    "\n",
    "**Disaster types:** Wildfires, floods, cyclones/typhoons, earthquakes\n",
    "\n",
    "**Species:** IUCN threatened (CR, EN, VU) Aves, Mammalia, Reptilia, Amphibia with ≥50 GBIF occurrences\n",
    "\n",
    "**Estimators:** DiD (TWFE), Sun-Abraham (staggered), Synthetic Control, Bayesian hierarchical\n",
    "\n",
    "**Outputs:** Event-study plots, coefficient tables, maps, species summaries, robustness checks, processed panels\n",
    "\n",
    "**Structure:**\n",
    "1. Setup & dependencies\n",
    "2. Configuration\n",
    "3. Data ingestion: GBIF, EM-DAT, IUCN, hazard footprints\n",
    "4. Preprocessing & grid alignment\n",
    "5. Treatment construction\n",
    "6. Outcome construction\n",
    "7. Causal estimation (DiD, SCM, Bayesian)\n",
    "8. Robustness & falsification\n",
    "9. Visualization & reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies (run once in your environment)\n",
    "# Uncomment and run as needed\n",
    "\n",
    "# Core spatial and data\n",
    "# !pip install geopandas rasterio xarray shapely pyproj rtree fiona\n",
    "# !pip install pandas numpy matplotlib seaborn requests tqdm joblib\n",
    "\n",
    "# GBIF API\n",
    "# !pip install pygbif\n",
    "\n",
    "# Statistical modeling\n",
    "# !pip install statsmodels linearmodels scikit-learn\n",
    "\n",
    "# Causal inference\n",
    "# !pip install dowhy econml pydid\n",
    "\n",
    "# Bayesian modeling (optional)\n",
    "# !pip install pymc arviz\n",
    "\n",
    "# Synthetic control (optional - or use R via rpy2)\n",
    "# !pip install synthdid\n",
    "\n",
    "# For reports\n",
    "# !pip install jupyter nbconvert plotly\n",
    "\n",
    "print(\"Dependencies listed. Uncomment and install as needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee280f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set for endangered_asia_disaster_analysis\n",
      "Study region: [60.0, -10.0, 150.0, 55.0]\n",
      "Time range: (2000, 2024)\n",
      "Grid resolution: 0.1°\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\appoo\\AppData\\Local\\Temp\\ipykernel_22184\\2685328261.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'notebook_generated_on': datetime.datetime.utcnow().isoformat() + 'Z',\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'study_name': 'endangered_asia_disaster_analysis',\n",
       " 'data_dir': WindowsPath('data'),\n",
       " 'results_dir': WindowsPath('results'),\n",
       " 'notebook_generated_on': '2025-11-05T19:09:57.866971Z',\n",
       " 'grid_resolution_deg': 0.1,\n",
       " 'region_bbox': [60.0, -10.0, 150.0, 55.0],\n",
       " 'time_unit': 'year',\n",
       " 'time_range': (2000, 2024),\n",
       " 'taxa': ['Aves', 'Mammalia', 'Reptilia', 'Amphibia'],\n",
       " 'target_status': ['CR', 'EN', 'VU'],\n",
       " 'min_occurrences': 50,\n",
       " 'disaster_types': ['wildfire', 'flood', 'cyclone', 'earthquake'],\n",
       " 'gbif_user': 'advaithmagic',\n",
       " 'gbif_password': 'gbifADA*1843',\n",
       " 'gbif_email': 'advaithsanilkumar@gmail.com',\n",
       " 'iucn_token': None,\n",
       " 'event_window': (-3, 5),\n",
       " 'clustering_var': 'grid_id',\n",
       " 'did_estimator': 'twfe',\n",
       " 'save_intermediate': True,\n",
       " 'figure_format': 'png',\n",
       " 'figure_dpi': 300}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Configuration — project parameters\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "CONFIG = {\n",
    "    'study_name': 'endangered_asia_disaster_analysis',\n",
    "    'data_dir': Path('data'),  # relative to notebook location\n",
    "    'results_dir': Path('results'),\n",
    "    'notebook_generated_on': datetime.datetime.utcnow().isoformat() + 'Z',\n",
    "    \n",
    "    # Spatial parameters\n",
    "    'grid_resolution_deg': 0.1,\n",
    "    'region_bbox': [60.0, -10.0, 150.0, 55.0],  # [min_lon, min_lat, max_lon, max_lat]\n",
    "    \n",
    "    # Temporal parameters\n",
    "    'time_unit': 'year',\n",
    "    'time_range': (2000, 2024),\n",
    "    \n",
    "    # Species selection\n",
    "    'taxa': ['Aves', 'Mammalia', 'Reptilia', 'Amphibia'],\n",
    "    'target_status': ['CR', 'EN', 'VU'],\n",
    "    'min_occurrences': 50,  # minimum GBIF records per species\n",
    "    \n",
    "    # Disaster types\n",
    "    'disaster_types': ['wildfire', 'flood', 'cyclone', 'earthquake'],\n",
    "    \n",
    "    # API credentials (fill these in)\n",
    "    'gbif_user': 'advaithmagic',  # GBIF username\n",
    "    'gbif_password': 'gbifADA*1843',  # GBIF password\n",
    "    'gbif_email': 'advaithsanilkumar@gmail.com',  # Email for download notifications\n",
    "    'iucn_token': None,  # IUCN API token (if using API)\n",
    "    \n",
    "    # Analysis parameters\n",
    "    'event_window': (-3, 5),  # years before/after for event study\n",
    "    'clustering_var': 'grid_id',  # for standard errors\n",
    "    'did_estimator': 'twfe',  # 'twfe', 'sun_abraham', or 'both'\n",
    "    \n",
    "    # Output formats\n",
    "    'save_intermediate': True,\n",
    "    'figure_format': 'png',\n",
    "    'figure_dpi': 300,\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [CONFIG['data_dir'], CONFIG['results_dir']]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    (dir_path / 'raw').mkdir(exist_ok=True)\n",
    "    (dir_path / 'processed').mkdir(exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(CONFIG['results_dir'] / 'config.json', 'w') as f:\n",
    "    json.dump({k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()}, f, indent=2)\n",
    "\n",
    "print(f\"Configuration set for {CONFIG['study_name']}\")\n",
    "print(f\"Study region: {CONFIG['region_bbox']}\")\n",
    "print(f\"Time range: {CONFIG['time_range']}\")\n",
    "print(f\"Grid resolution: {CONFIG['grid_resolution_deg']}°\")\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd642b",
   "metadata": {},
   "source": [
    "## 3. Data Sources Overview\n",
    "\n",
    "**EM-DAT (CRED):** Global disaster records - requires registration at https://www.emdat.be/\n",
    "- Download disasters for Asia, 2000-2024\n",
    "- Filter by disaster types: wildfire, flood, storm, earthquake\n",
    "- Save as `data/raw/emdat_asia_2000_2024.csv`\n",
    "\n",
    "**GBIF:** Species occurrence data via API\n",
    "- Implemented below with bulk download workflow\n",
    "- Requires GBIF account (free): https://www.gbif.org/\n",
    "\n",
    "**IUCN Red List:** Species range maps\n",
    "- Download from https://www.iucnredlist.org/resources/spatial-data-download\n",
    "- Requires acceptance of terms\n",
    "- Save shapefiles to `data/raw/iucn/`\n",
    "\n",
    "**Hazard-specific footprints:**\n",
    "- MODIS/VIIRS burned area: https://modis-fire.umd.edu/\n",
    "- Copernicus EMS flood footprints: https://emergency.copernicus.eu/\n",
    "- USGS earthquake data: https://earthquake.usgs.gov/\n",
    "- NASA GPM IMERG precipitation: https://gpm.nasa.gov/\n",
    "\n",
    "**MODIS MCD12 land cover:** Annual global land cover at 500m\n",
    "- Download from https://lpdaac.usgs.gov/products/mcd12q1v006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccb954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Aves species...\n",
      "  Found taxon key: 212\n",
      "Fetching Mammalia species...\n",
      "  Found taxon key: 212\n",
      "Fetching Mammalia species...\n",
      "  Found taxon key: 359\n",
      "Fetching Reptilia species...\n",
      "  Found taxon key: 359\n",
      "Fetching Reptilia species...\n",
      "  Found taxon key: 12170550\n",
      "Fetching Amphibia species...\n",
      "  Found taxon key: 12170550\n",
      "Fetching Amphibia species...\n",
      "  Found taxon key: 131\n",
      "\n",
      "NOTE: This is a placeholder. For production:\n",
      "1. Use IUCN Red List API to get threatened species list\n",
      "2. Or manually download species list from IUCN website\n",
      "3. Cross-reference with GBIF taxonomy\n",
      "\n",
      "Species to analyze: 4\n",
      "  Found taxon key: 131\n",
      "\n",
      "NOTE: This is a placeholder. For production:\n",
      "1. Use IUCN Red List API to get threatened species list\n",
      "2. Or manually download species list from IUCN website\n",
      "3. Cross-reference with GBIF taxonomy\n",
      "\n",
      "Species to analyze: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxon</th>\n",
       "      <th>scientificName</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aves</td>\n",
       "      <td>Lophura edwardsi</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mammalia</td>\n",
       "      <td>Panthera tigris</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reptilia</td>\n",
       "      <td>Crocodylus siamensis</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amphibia</td>\n",
       "      <td>Ansonia latidisca</td>\n",
       "      <td>EN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      taxon        scientificName status\n",
       "0      Aves      Lophura edwardsi     CR\n",
       "1  Mammalia       Panthera tigris     EN\n",
       "2  Reptilia  Crocodylus siamensis     CR\n",
       "3  Amphibia     Ansonia latidisca     EN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4a. GBIF species list acquisition - get threatened species in Asia\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pygbif import species as gbif_species\n",
    "from pygbif import occurrences as gbif_occ\n",
    "import time\n",
    "\n",
    "def get_iucn_threatened_species_asia(taxa_list, status_list, bbox, iucn_token=None):\n",
    "    \"\"\"\n",
    "    Query IUCN Red List API for threatened species in Asia.\n",
    "    If no API token, returns a curated list of example species.\n",
    "    \n",
    "    For production: Get API token from https://apiv3.iucnredlist.org/api/v3/token\n",
    "    \"\"\"\n",
    "    species_list = []\n",
    "    \n",
    "    if iucn_token:\n",
    "        # Use IUCN API\n",
    "        base_url = \"https://apiv3.iucnredlist.org/api/v3\"\n",
    "        headers = {'token': iucn_token}\n",
    "        \n",
    "        for taxon in taxa_list:\n",
    "            for status in status_list:\n",
    "                try:\n",
    "                    # Get species by category and taxon\n",
    "                    url = f\"{base_url}/species/category/{status}\"\n",
    "                    response = requests.get(url, headers=headers, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        for sp in data.get('result', []):\n",
    "                            if sp.get('class_name', '').lower() == taxon.lower():\n",
    "                                species_list.append({\n",
    "                                    'taxon': taxon,\n",
    "                                    'scientificName': sp.get('scientific_name'),\n",
    "                                    'status': status,\n",
    "                                    'taxonid': sp.get('taxonid')\n",
    "                                })\n",
    "                    time.sleep(0.5)  # Rate limiting\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching {taxon} - {status}: {e}\")\n",
    "    else:\n",
    "        print(\"No IUCN token provided. Using curated example species list.\")\n",
    "        print(\"For production: Set CONFIG['iucn_token'] with your API token\")\n",
    "        \n",
    "        # Curated list of threatened species in Asia (examples from each taxonomic group)\n",
    "        species_list = [\n",
    "            # Aves (Birds)\n",
    "            {'taxon': 'Aves', 'scientificName': 'Lophura edwardsi', 'status': 'CR'},\n",
    "            {'taxon': 'Aves', 'scientificName': 'Arborophila davidi', 'status': 'EN'},\n",
    "            {'taxon': 'Aves', 'scientificName': 'Turdoides striata', 'status': 'VU'},\n",
    "            {'taxon': 'Aves', 'scientificName': 'Carpococcyx renauldi', 'status': 'EN'},\n",
    "            \n",
    "            # Mammalia (Mammals)\n",
    "            {'taxon': 'Mammalia', 'scientificName': 'Panthera tigris', 'status': 'EN'},\n",
    "            {'taxon': 'Mammalia', 'scientificName': 'Elephas maximus', 'status': 'EN'},\n",
    "            {'taxon': 'Mammalia', 'scientificName': 'Rhinoceros sondaicus', 'status': 'CR'},\n",
    "            {'taxon': 'Mammalia', 'scientificName': 'Pongo abelii', 'status': 'CR'},\n",
    "            \n",
    "            # Reptilia (Reptiles)\n",
    "            {'taxon': 'Reptilia', 'scientificName': 'Crocodylus siamensis', 'status': 'CR'},\n",
    "            {'taxon': 'Reptilia', 'scientificName': 'Chelonia mydas', 'status': 'EN'},\n",
    "            {'taxon': 'Reptilia', 'scientificName': 'Cuora trifasciata', 'status': 'CR'},\n",
    "            \n",
    "            # Amphibia (Amphibians)\n",
    "            {'taxon': 'Amphibia', 'scientificName': 'Ansonia latidisca', 'status': 'EN'},\n",
    "            {'taxon': 'Amphibia', 'scientificName': 'Rhacophorus catamitus', 'status': 'VU'},\n",
    "        ]\n",
    "    \n",
    "    return pd.DataFrame(species_list)\n",
    "\n",
    "# Get species list\n",
    "species_df = get_iucn_threatened_species_asia(\n",
    "    CONFIG['taxa'], \n",
    "    CONFIG['target_status'],\n",
    "    CONFIG['region_bbox'],\n",
    "    iucn_token=CONFIG['iucn_token']\n",
    ")\n",
    "\n",
    "print(f\"\\nSpecies to analyze: {len(species_df)}\")\n",
    "print(f\"Taxonomic groups: {species_df['taxon'].value_counts().to_dict()}\")\n",
    "print(f\"Conservation status: {species_df['status'].value_counts().to_dict()}\")\n",
    "\n",
    "# Save species list\n",
    "species_df.to_csv(CONFIG['data_dir'] / 'processed' / 'target_species.csv', index=False)\n",
    "print(f\"\\nSaved species list to {CONFIG['data_dir'] / 'processed' / 'target_species.csv'}\")\n",
    "\n",
    "species_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d624c648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBIF Download Workflow:\n",
      "============================================================\n",
      "\n",
      "Step 1: Request download for 4 species\n",
      "Uncomment the line below to initiate download:\n",
      "# download_key = request_gbif_download(species_names, CONFIG['time_range'][0], CONFIG['time_range'][1], CONFIG['region_bbox'], CONFIG['gbif_user'], CONFIG['gbif_password'], CONFIG['gbif_email'])\n",
      "\n",
      "Step 2: After receiving email, download data:\n",
      "# gbif_csv = download_gbif_data(download_key, CONFIG['gbif_user'], CONFIG['gbif_password'], CONFIG['data_dir'] / 'raw' / 'gbif')\n",
      "\n",
      "Expected GBIF data path: data\\raw\\gbif\\occurrences.csv\n"
     ]
    }
   ],
   "source": [
    "# 4b. GBIF bulk occurrence download workflow\n",
    "from pygbif import occurrences as occ\n",
    "import time\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def request_gbif_download(species_list, year_min, year_max, bbox, user, pwd, email):\n",
    "    \"\"\"\n",
    "    Request bulk occurrence download from GBIF.\n",
    "    Returns download key for later retrieval.\n",
    "    \"\"\"\n",
    "    if not all([user, pwd, email]):\n",
    "        print(\"ERROR: GBIF credentials required for bulk download\")\n",
    "        print(\"Set CONFIG['gbif_user'], CONFIG['gbif_password'], CONFIG['gbif_email']\")\n",
    "        return None\n",
    "    \n",
    "    # Get taxon keys for species\n",
    "    taxon_keys = []\n",
    "    for sp_name in species_list:\n",
    "        try:\n",
    "            result = gbif_species.name_backbone(name=sp_name)\n",
    "            if result.get('usageKey'):\n",
    "                taxon_keys.append(str(result['usageKey']))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if not taxon_keys:\n",
    "        print(\"No valid taxon keys found for species list\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(taxon_keys)} taxon keys for {len(species_list)} species\")\n",
    "    \n",
    "    # Build download predicate\n",
    "    try:\n",
    "        download_key = occ.download([\n",
    "            f\"taxonKey = {','.join(taxon_keys)}\",\n",
    "            \"hasCoordinate = true\",\n",
    "            \"hasGeospatialIssue = false\",\n",
    "            f\"year >= {year_min}\",\n",
    "            f\"year <= {year_max}\",\n",
    "            f\"decimalLatitude >= {bbox[1]}\",\n",
    "            f\"decimalLatitude <= {bbox[3]}\",\n",
    "            f\"decimalLongitude >= {bbox[0]}\",\n",
    "            f\"decimalLongitude <= {bbox[2]}\"\n",
    "        ], user=user, pwd=pwd, email=email)\n",
    "        \n",
    "        print(f\"Download requested. Key: {download_key}\")\n",
    "        print(f\"You will receive an email at {email} when ready\")\n",
    "        print(f\"Download URL: https://www.gbif.org/occurrence/download/{download_key}\")\n",
    "        \n",
    "        return download_key\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error requesting download: {e}\")\n",
    "        print(\"\\nAlternative: Use GBIF web interface:\")\n",
    "        print(\"1. Go to https://www.gbif.org/occurrence/search\")\n",
    "        print(\"2. Filter by species, date range, and coordinates\")\n",
    "        print(\"3. Click 'Download' and select 'Simple CSV'\")\n",
    "        print(f\"4. Save to {CONFIG['data_dir'] / 'raw' / 'gbif' / 'occurrences.csv'}\")\n",
    "        return None\n",
    "\n",
    "def download_gbif_data(download_key, user, pwd, output_dir):\n",
    "    \"\"\"\n",
    "    Download and extract GBIF occurrence data once ready.\n",
    "    \"\"\"\n",
    "    if not download_key:\n",
    "        print(\"No download key provided\")\n",
    "        return None\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Check download status\n",
    "        metadata = occ.download_meta(download_key)\n",
    "        status = metadata.get('status', 'UNKNOWN')\n",
    "        \n",
    "        print(f\"Download status: {status}\")\n",
    "        \n",
    "        if status != 'SUCCEEDED':\n",
    "            print(f\"Download not ready yet. Current status: {status}\")\n",
    "            print(\"Please wait for email notification or check status later\")\n",
    "            return None\n",
    "        \n",
    "        # Download the file\n",
    "        print(\"Downloading data...\")\n",
    "        zip_path = output_dir / f\"{download_key}.zip\"\n",
    "        \n",
    "        result = occ.download_get(download_key, path=str(output_dir))\n",
    "        \n",
    "        # Extract\n",
    "        print(\"Extracting data...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "        \n",
    "        # Find the occurrence file\n",
    "        occurrence_file = output_dir / 'occurrence.txt'\n",
    "        if occurrence_file.exists():\n",
    "            # Rename to CSV\n",
    "            output_csv = output_dir / 'occurrences.csv'\n",
    "            occurrence_file.rename(output_csv)\n",
    "            print(f\"Success! Data saved to {output_csv}\")\n",
    "            return output_csv\n",
    "        else:\n",
    "            print(\"Could not find occurrence.txt in download\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if CONFIG['gbif_email']:\n",
    "    print(\"GBIF Download Workflow:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get species names\n",
    "    if 'species_df' in locals() and not species_df.empty:\n",
    "        species_names = species_df['scientificName'].tolist()\n",
    "        \n",
    "        print(f\"\\nStep 1: Request download for {len(species_names)} species\")\n",
    "        print(\"Uncomment the line below to initiate download:\")\n",
    "        print(\"# download_key = request_gbif_download(species_names, CONFIG['time_range'][0], CONFIG['time_range'][1], CONFIG['region_bbox'], CONFIG['gbif_user'], CONFIG['gbif_password'], CONFIG['gbif_email'])\")\n",
    "        \n",
    "        print(\"\\nStep 2: After receiving email, download data:\")\n",
    "        print(\"# gbif_csv = download_gbif_data(download_key, CONFIG['gbif_user'], CONFIG['gbif_password'], CONFIG['data_dir'] / 'raw' / 'gbif')\")\n",
    "    else:\n",
    "        print(\"First run cell 4a to get species list\")\n",
    "else:\n",
    "    print(\"Please set CONFIG['gbif_email'] to use GBIF download workflow\")\n",
    "    print(\"\\nAlternative: Use GBIF web interface and place CSV in data/raw/gbif/\")\n",
    "\n",
    "gbif_raw_path = CONFIG['data_dir'] / 'raw' / 'gbif' / 'occurrences.csv'\n",
    "print(f\"\\nExpected GBIF data path: {gbif_raw_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcf6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['DisNo.', 'Historic', 'Classification Key', 'Disaster Group', 'Disaster Subgroup', 'Disaster Type', 'Disaster Subtype', 'External IDs', 'Event Name', 'ISO'] ...\n",
      "Loaded 693 disaster events from 2000 to 2024\n",
      "\n",
      "EM-DAT data summary:\n",
      "disaster_type\n",
      "Earthquake    324\n",
      "Flood         315\n",
      "Storm          54\n",
      "dtype: int64\n",
      "\n",
      "Years covered: 2000 to 2024\n",
      "Events with coordinates: 693\n"
     ]
    }
   ],
   "source": [
    "# 4c. EM-DAT disaster data ingestion\n",
    "import pandas as pd\n",
    "\n",
    "def load_emdat_data(filepath, disaster_types, year_min, year_max, bbox):\n",
    "    \"\"\"\n",
    "    Load and filter EM-DAT disaster data.\n",
    "    Expected columns from EM-DAT: Start Year, Disaster Type, Country, Latitude, Longitude, etc.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"ERROR: EM-DAT file not found at {filepath}\")\n",
    "        print(\"Download from https://www.emdat.be/ and place in data/raw/\")\n",
    "        return None\n",
    "    \n",
    "    # Load with encoding that handles special characters\n",
    "    df = pd.read_csv(filepath, encoding='utf-8-sig', low_memory=False)\n",
    "    \n",
    "    # Print column names for debugging\n",
    "    print(\"Available columns:\", df.columns.tolist()[:10], \"...\")\n",
    "    \n",
    "    # The actual EM-DAT file uses 'Start Year' not 'year'\n",
    "    # and 'Disaster Type' not 'disaster_type'\n",
    "    \n",
    "    # Filter by year - use 'Start Year' column\n",
    "    if 'Start Year' in df.columns:\n",
    "        df = df[(df['Start Year'] >= year_min) & (df['Start Year'] <= year_max)]\n",
    "    else:\n",
    "        print(\"WARNING: 'Start Year' column not found\")\n",
    "        return None\n",
    "    \n",
    "    # Filter by disaster type - map to EM-DAT disaster types\n",
    "    disaster_map = {\n",
    "        'wildfire': ['Wildfire'],\n",
    "        'flood': ['Flood', 'Flash flood', 'Riverine flood', 'Coastal flood'],\n",
    "        'cyclone': ['Storm', 'Tropical cyclone', 'Typhoon', 'Cyclone'],\n",
    "        'earthquake': ['Earthquake', 'Ground movement']\n",
    "    }\n",
    "    \n",
    "    type_filter = []\n",
    "    for dtype in disaster_types:\n",
    "        if dtype in disaster_map:\n",
    "            type_filter.extend(disaster_map[dtype])\n",
    "    \n",
    "    # Use 'Disaster Subtype' or 'Disaster Type' column\n",
    "    if 'Disaster Subtype' in df.columns:\n",
    "        df = df[df['Disaster Subtype'].isin(type_filter)]\n",
    "    elif 'Disaster Type' in df.columns:\n",
    "        df = df[df['Disaster Type'].isin(type_filter)]\n",
    "    else:\n",
    "        print(\"WARNING: Disaster type columns not found\")\n",
    "    \n",
    "    # Filter by geography (if coordinates available)\n",
    "    if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "        # Remove rows with missing coordinates\n",
    "        df = df.dropna(subset=['Latitude', 'Longitude'])\n",
    "        df = df[\n",
    "            (df['Longitude'] >= bbox[0]) & (df['Longitude'] <= bbox[2]) &\n",
    "            (df['Latitude'] >= bbox[1]) & (df['Latitude'] <= bbox[3])\n",
    "        ]\n",
    "    else:\n",
    "        print(\"WARNING: Latitude/Longitude columns not found - filtering by country names instead\")\n",
    "        # Fallback: filter by Asian countries if coordinates not available\n",
    "        # You may need to expand this list\n",
    "        asian_countries = ['China', 'India', 'Indonesia', 'Pakistan', 'Bangladesh', \n",
    "                          'Japan', 'Philippines', 'Vietnam', 'Thailand', 'Myanmar',\n",
    "                          'Korea', 'Nepal', 'Sri Lanka', 'Malaysia', 'Cambodia']\n",
    "        if 'Country' in df.columns:\n",
    "            df = df[df['Country'].isin(asian_countries)]\n",
    "    \n",
    "    # Standardize column names for downstream use\n",
    "    column_mapping = {\n",
    "        'Start Year': 'year',\n",
    "        'Disaster Type': 'disaster_type',\n",
    "        'Disaster Subtype': 'disaster_subtype',\n",
    "        'Latitude': 'latitude',\n",
    "        'Longitude': 'longitude',\n",
    "        'Total Deaths': 'total_deaths',\n",
    "        'No. Affected': 'affected',\n",
    "        'Total Damage (\\'000 US$)': 'damage_usd'\n",
    "    }\n",
    "    \n",
    "    # Rename columns that exist\n",
    "    rename_dict = {k: v for k, v in column_mapping.items() if k in df.columns}\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} disaster events from {year_min} to {year_max}\")\n",
    "    return df\n",
    "\n",
    "emdat_path = CONFIG['data_dir'] / 'raw' / 'emdat_asia_2000_2024.csv'\n",
    "emdat_df = load_emdat_data(\n",
    "    emdat_path,\n",
    "    CONFIG['disaster_types'],\n",
    "    CONFIG['time_range'][0],\n",
    "    CONFIG['time_range'][1],\n",
    "    CONFIG['region_bbox']\n",
    ")\n",
    "\n",
    "if emdat_df is not None:\n",
    "    print(\"\\nEM-DAT data summary:\")\n",
    "    if 'disaster_type' in emdat_df.columns:\n",
    "        print(emdat_df.groupby('disaster_type').size())\n",
    "    elif 'disaster_subtype' in emdat_df.columns:\n",
    "        print(emdat_df.groupby('disaster_subtype').size())\n",
    "    \n",
    "    print(f\"\\nYears covered: {emdat_df['year'].min()} to {emdat_df['year'].max()}\")\n",
    "    print(f\"Events with coordinates: {emdat_df[['latitude', 'longitude']].notna().all(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce6a808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MAMMALS data...\n",
      "  Loading MAMMALS_PART1.shp...\n",
      "  Loading MAMMALS_PART2.shp...\n",
      "  Loading MAMMALS_PART2.shp...\n",
      "Loading BIRDS data from GPKG...\n",
      "Loading BIRDS data from GPKG...\n",
      "Loading REPTILES data...\n",
      "  Loading REPTILES_PART1.shp...\n",
      "Loading REPTILES data...\n",
      "  Loading REPTILES_PART1.shp...\n",
      "  Loading REPTILES_PART2.shp...\n",
      "  Loading REPTILES_PART2.shp...\n",
      "Loading AMPHIBIANS data...\n",
      "  Loading AMPHIBIANS_PART1.shp...\n",
      "Loading AMPHIBIANS data...\n",
      "  Loading AMPHIBIANS_PART1.shp...\n",
      "  Loading AMPHIBIANS_PART2.shp...\n",
      "  Loading AMPHIBIANS_PART2.shp...\n",
      "Loaded 7 range polygons for 4 species\n",
      "Loaded 7 range polygons for 4 species\n"
     ]
    }
   ],
   "source": [
    "# 4d. IUCN range map processing\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_iucn_ranges(iucn_dir, species_list):\n",
    "    \"\"\"\n",
    "    Load IUCN range shapefiles for species in species_list.\n",
    "    Handles PART1/PART2 shapefiles for mammals/reptiles/amphibians and GPKG for birds.\n",
    "    \"\"\"\n",
    "    if not iucn_dir.exists():\n",
    "        print(f\"ERROR: IUCN directory not found at {iucn_dir}\")\n",
    "        print(\"Download from https://www.iucnredlist.org/resources/spatial-data-download\")\n",
    "        return None\n",
    "    \n",
    "    gdfs = []\n",
    "    \n",
    "    # Load MAMMALS (PART1 and PART2 shapefiles)\n",
    "    mammals_dir = iucn_dir / 'MAMMALS'\n",
    "    if mammals_dir.exists():\n",
    "        print(\"Loading MAMMALS data...\")\n",
    "        for part in ['PART1', 'PART2']:\n",
    "            shp_file = mammals_dir / f'MAMMALS_{part}.shp'\n",
    "            if shp_file.exists():\n",
    "                print(f\"  Loading MAMMALS_{part}.shp...\")\n",
    "                gdf = gpd.read_file(shp_file)\n",
    "                # Filter to species of interest\n",
    "                if 'binomial' in gdf.columns:\n",
    "                    gdf = gdf[gdf['binomial'].isin(species_list)]\n",
    "                    gdfs.append(gdf)\n",
    "                elif 'sci_name' in gdf.columns:\n",
    "                    gdf = gdf[gdf['sci_name'].isin(species_list)]\n",
    "                    gdfs.append(gdf)\n",
    "    \n",
    "    # Load BIRDS (GPKG format)\n",
    "    birds_dir = iucn_dir / 'BIRDS'\n",
    "    if birds_dir.exists():\n",
    "        gpkg_file = birds_dir / 'BOTW_2025.gpkg'\n",
    "        if gpkg_file.exists():\n",
    "            print(\"Loading BIRDS data from GPKG...\")\n",
    "            gdf = gpd.read_file(gpkg_file)\n",
    "            # Filter to species of interest - birds typically use 'sci_name' or 'SCINAME'\n",
    "            name_col = None\n",
    "            for col in ['sci_name', 'SCINAME', 'binomial', 'scientific_name']:\n",
    "                if col in gdf.columns:\n",
    "                    name_col = col\n",
    "                    break\n",
    "            if name_col:\n",
    "                gdf = gdf[gdf[name_col].isin(species_list)]\n",
    "                gdfs.append(gdf)\n",
    "    \n",
    "    # Load REPTILES (PART1 and PART2 shapefiles)\n",
    "    reptiles_dir = iucn_dir / 'REPTILES'\n",
    "    if reptiles_dir.exists():\n",
    "        print(\"Loading REPTILES data...\")\n",
    "        for part in ['PART1', 'PART2']:\n",
    "            shp_file = reptiles_dir / f'REPTILES_{part}.shp'\n",
    "            if shp_file.exists():\n",
    "                print(f\"  Loading REPTILES_{part}.shp...\")\n",
    "                gdf = gpd.read_file(shp_file)\n",
    "                # Filter to species of interest\n",
    "                if 'binomial' in gdf.columns:\n",
    "                    gdf = gdf[gdf['binomial'].isin(species_list)]\n",
    "                    gdfs.append(gdf)\n",
    "                elif 'sci_name' in gdf.columns:\n",
    "                    gdf = gdf[gdf['sci_name'].isin(species_list)]\n",
    "                    gdfs.append(gdf)\n",
    "    \n",
    "    # Load AMPHIBIANS (PART1 and PART2 shapefiles)\n",
    "    amphibians_dir = iucn_dir / 'AMPHIBIANS'\n",
    "    if amphibians_dir.exists():\n",
    "        print(\"Loading AMPHIBIANS data...\")\n",
    "        for part in ['PART1', 'PART2']:\n",
    "            shp_file = amphibians_dir / f'AMPHIBIANS_{part}.shp'\n",
    "            if shp_file.exists():\n",
    "                print(f\"  Loading AMPHIBIANS_{part}.shp...\")\n",
    "                gdf = gpd.read_file(shp_file)\n",
    "                # Filter to species of interest\n",
    "                if 'binomial' in gdf.columns:\n",
    "                    gdf = gdf[gdf['binomial'].isin(species_list)]\n",
    "                    gdfs.append(gdf)\n",
    "                elif 'sci_name' in gdf.columns:\n",
    "                    gdf = gdf[gdf['sci_name'].isin(species_list)]\n",
    "                    gdfs.append(gdf)\n",
    "    \n",
    "    if not gdfs:\n",
    "        print(\"No matching species found in IUCN data\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all GeoDataFrames\n",
    "    ranges = pd.concat(gdfs, ignore_index=True)\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'sci_name' in ranges.columns and 'binomial' not in ranges.columns:\n",
    "        ranges['binomial'] = ranges['sci_name']\n",
    "    \n",
    "    print(f\"Loaded {len(ranges)} range polygons for {ranges['binomial'].nunique()} species\")\n",
    "    \n",
    "    return ranges\n",
    "\n",
    "iucn_dir = CONFIG['data_dir'] / 'raw' / 'iucn'\n",
    "species_names = species_df['scientificName'].tolist() if 'species_df' in locals() else []\n",
    "\n",
    "iucn_ranges = load_iucn_ranges(iucn_dir, species_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3dbbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Created 7 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved IUCN ranges to data\\processed\\iucn_ranges_filtered.gpkg\n",
      "Saved 7 range polygons for 4 species\n"
     ]
    }
   ],
   "source": [
    "# 4e. Save processed IUCN ranges\n",
    "if iucn_ranges is not None:\n",
    "    # Save as GeoPackage (recommended format - single file, fast, supports all geometry types)\n",
    "    iucn_processed_path = CONFIG['data_dir'] / 'processed' / 'iucn_ranges_filtered.gpkg'\n",
    "    iucn_ranges.to_file(iucn_processed_path, driver='GPKG')\n",
    "    print(f\"Saved IUCN ranges to {iucn_processed_path}\")\n",
    "    \n",
    "    # Alternative: Save as Shapefile (if needed for compatibility)\n",
    "    # Note: Shapefiles have column name length limitations (10 chars)\n",
    "    # iucn_shp_path = CONFIG['data_dir'] / 'processed' / 'iucn_ranges_filtered.shp'\n",
    "    # iucn_ranges.to_file(iucn_shp_path, driver='ESRI Shapefile')\n",
    "    \n",
    "    # Alternative: Save as GeoJSON (human-readable, larger file size)\n",
    "    # iucn_geojson_path = CONFIG['data_dir'] / 'processed' / 'iucn_ranges_filtered.geojson'\n",
    "    # iucn_ranges.to_file(iucn_geojson_path, driver='GeoJSON')\n",
    "    \n",
    "    print(f\"Saved {len(iucn_ranges)} range polygons for {iucn_ranges['binomial'].nunique()} species\")\n",
    "else:\n",
    "    print(\"No IUCN ranges to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e04f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grid with 585,000 cells\n",
      "Avg cell area: 145.2 km²\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Created 585,000 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved grid to data\\processed\\analysis_grid.gpkg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid_id</th>\n",
       "      <th>lon_center</th>\n",
       "      <th>lat_center</th>\n",
       "      <th>geometry</th>\n",
       "      <th>area_km2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g_0_0</td>\n",
       "      <td>60.05</td>\n",
       "      <td>-9.95</td>\n",
       "      <td>POLYGON ((60.1 -10, 60.1 -9.9, 60 -9.9, 60 -10...</td>\n",
       "      <td>125.812666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g_0_1</td>\n",
       "      <td>60.05</td>\n",
       "      <td>-9.85</td>\n",
       "      <td>POLYGON ((60.1 -9.9, 60.1 -9.8, 60 -9.8, 60 -9...</td>\n",
       "      <td>125.774348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g_0_2</td>\n",
       "      <td>60.05</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>POLYGON ((60.1 -9.8, 60.1 -9.7, 60 -9.7, 60 -9...</td>\n",
       "      <td>125.736437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g_0_3</td>\n",
       "      <td>60.05</td>\n",
       "      <td>-9.65</td>\n",
       "      <td>POLYGON ((60.1 -9.7, 60.1 -9.6, 60 -9.6, 60 -9...</td>\n",
       "      <td>125.698931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g_0_4</td>\n",
       "      <td>60.05</td>\n",
       "      <td>-9.55</td>\n",
       "      <td>POLYGON ((60.1 -9.6, 60.1 -9.5, 60 -9.5, 60 -9...</td>\n",
       "      <td>125.661830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grid_id  lon_center  lat_center  \\\n",
       "0   g_0_0       60.05       -9.95   \n",
       "1   g_0_1       60.05       -9.85   \n",
       "2   g_0_2       60.05       -9.75   \n",
       "3   g_0_3       60.05       -9.65   \n",
       "4   g_0_4       60.05       -9.55   \n",
       "\n",
       "                                            geometry    area_km2  \n",
       "0  POLYGON ((60.1 -10, 60.1 -9.9, 60 -9.9, 60 -10...  125.812666  \n",
       "1  POLYGON ((60.1 -9.9, 60.1 -9.8, 60 -9.8, 60 -9...  125.774348  \n",
       "2  POLYGON ((60.1 -9.8, 60.1 -9.7, 60 -9.7, 60 -9...  125.736437  \n",
       "3  POLYGON ((60.1 -9.7, 60.1 -9.6, 60 -9.6, 60 -9...  125.698931  \n",
       "4  POLYGON ((60.1 -9.6, 60.1 -9.5, 60 -9.5, 60 -9...  125.661830  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Create spatial analysis grid\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "def create_analysis_grid(bbox, resolution):\n",
    "    \"\"\"\n",
    "    Create regular lat-lon grid covering bounding box.\n",
    "    Returns GeoDataFrame with grid cells.\n",
    "    \"\"\"\n",
    "    min_lon, min_lat, max_lon, max_lat = bbox\n",
    "    \n",
    "    lons = np.arange(min_lon, max_lon, resolution)\n",
    "    lats = np.arange(min_lat, max_lat, resolution)\n",
    "    \n",
    "    grid_polys = []\n",
    "    grid_ids = []\n",
    "    grid_centers = []\n",
    "    \n",
    "    for i, lon in enumerate(lons):\n",
    "        for j, lat in enumerate(lats):\n",
    "            poly = box(lon, lat, lon + resolution, lat + resolution)\n",
    "            grid_polys.append(poly)\n",
    "            grid_ids.append(f'g_{i}_{j}')\n",
    "            grid_centers.append((lon + resolution/2, lat + resolution/2))\n",
    "    \n",
    "    grid_gdf = gpd.GeoDataFrame({\n",
    "        'grid_id': grid_ids,\n",
    "        'lon_center': [c[0] for c in grid_centers],\n",
    "        'lat_center': [c[1] for c in grid_centers],\n",
    "        'geometry': grid_polys\n",
    "    }, crs='EPSG:4326')\n",
    "    \n",
    "    # Calculate grid area in km²\n",
    "    grid_gdf['area_km2'] = grid_gdf.to_crs('EPSG:3857').geometry.area / 1e6\n",
    "    \n",
    "    return grid_gdf\n",
    "\n",
    "grid_gdf = create_analysis_grid(CONFIG['region_bbox'], CONFIG['grid_resolution_deg'])\n",
    "print(f\"Created grid with {len(grid_gdf):,} cells\")\n",
    "print(f\"Avg cell area: {grid_gdf['area_km2'].mean():.1f} km²\")\n",
    "\n",
    "# Save grid for reference\n",
    "grid_path = CONFIG['data_dir'] / 'processed' / 'analysis_grid.gpkg'\n",
    "grid_gdf.to_file(grid_path, driver='GPKG')\n",
    "print(f\"Saved grid to {grid_path}\")\n",
    "\n",
    "grid_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15b1b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial join functions defined\n"
     ]
    }
   ],
   "source": [
    "# 6. Spatial join functions - link occurrences and disasters to grid\n",
    "import geopandas as gpd\n",
    "\n",
    "def map_points_to_grid(points_gdf, grid_gdf):\n",
    "    \"\"\"\n",
    "    Spatially join point data to grid cells.\n",
    "    \"\"\"\n",
    "    # Ensure same CRS\n",
    "    pts = points_gdf.to_crs(grid_gdf.crs)\n",
    "    \n",
    "    # Spatial join\n",
    "    joined = gpd.sjoin(pts, grid_gdf[['grid_id', 'geometry']], how='left', predicate='within')\n",
    "    \n",
    "    # Drop points outside grid\n",
    "    joined = joined.dropna(subset=['grid_id'])\n",
    "    \n",
    "    return joined\n",
    "\n",
    "def map_polygon_to_grid(poly_gdf, grid_gdf):\n",
    "    \"\"\"\n",
    "    Compute grid cell overlap with polygon ranges.\n",
    "    Returns grid cells with fraction of area covered.\n",
    "    \"\"\"\n",
    "    poly = poly_gdf.to_crs(grid_gdf.crs)\n",
    "    \n",
    "    # Intersection\n",
    "    intersected = gpd.overlay(poly, grid_gdf[['grid_id', 'geometry']], how='intersection')\n",
    "    \n",
    "    # Calculate overlap area\n",
    "    intersected['overlap_km2'] = intersected.to_crs('EPSG:3857').geometry.area / 1e6\n",
    "    \n",
    "    # Merge with grid areas to get fraction\n",
    "    intersected = intersected.merge(\n",
    "        grid_gdf[['grid_id', 'area_km2']],\n",
    "        on='grid_id',\n",
    "        how='left'\n",
    "    )\n",
    "    intersected['overlap_fraction'] = intersected['overlap_km2'] / intersected['area_km2']\n",
    "    \n",
    "    return intersected\n",
    "\n",
    "def expand_disaster_footprint(disaster_points, radius_km, grid_gdf):\n",
    "    \"\"\"\n",
    "    Expand point disasters to circular footprints.\n",
    "    Returns affected grid cells.\n",
    "    \"\"\"\n",
    "    from shapely.geometry import Point\n",
    "    \n",
    "    # Convert to projected CRS for buffering\n",
    "    pts = disaster_points.to_crs('EPSG:3857')\n",
    "    \n",
    "    # Buffer by radius\n",
    "    pts['geometry'] = pts.geometry.buffer(radius_km * 1000)\n",
    "    \n",
    "    # Back to WGS84\n",
    "    pts = pts.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Intersect with grid\n",
    "    affected = map_polygon_to_grid(pts, grid_gdf)\n",
    "    \n",
    "    return affected\n",
    "\n",
    "print(\"Spatial join functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3e2ce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBIF data not found at data\\raw\\gbif\\occurrences.csv\n",
      "\n",
      "To proceed:\n",
      "1. Run cells 4a and 4b to request GBIF download\n",
      "2. Wait for email notification from GBIF\n",
      "3. Place downloaded occurrence file at:\n",
      "   data\\raw\\gbif\\occurrences.csv\n",
      "4. Re-run this cell to process the data\n"
     ]
    }
   ],
   "source": [
    "# 7. Occurrence data processing and outcome construction\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_process_gbif_occurrences(filepath, species_list, year_min, year_max, bbox):\n",
    "    \"\"\"\n",
    "    Load GBIF occurrence CSV and prepare for analysis.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"ERROR: GBIF file not found at {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    # Load occurrences\n",
    "    df = pd.read_csv(filepath, sep='\\t', low_memory=False)\n",
    "    \n",
    "    # Standardize columns\n",
    "    required_cols = ['species', 'decimalLatitude', 'decimalLongitude', 'eventDate', 'year']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"ERROR: Missing required columns. Found: {df.columns.tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    # Filter by species\n",
    "    df = df[df['species'].isin(species_list)]\n",
    "    \n",
    "    # Filter by year\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "    df = df[(df['year'] >= year_min) & (df['year'] <= year_max)]\n",
    "    \n",
    "    # Filter by coordinates\n",
    "    df = df[\n",
    "        (df['decimalLongitude'] >= bbox[0]) & (df['decimalLongitude'] <= bbox[2]) &\n",
    "        (df['decimalLatitude'] >= bbox[1]) & (df['decimalLatitude'] <= bbox[3])\n",
    "    ]\n",
    "    \n",
    "    # Drop invalid coordinates\n",
    "    df = df.dropna(subset=['decimalLatitude', 'decimalLongitude'])\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} occurrences for {df['species'].nunique()} species\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def build_occurrence_panel(occurrence_df, grid_gdf, time_unit='year'):\n",
    "    \"\"\"\n",
    "    Create grid × time × species panel with occurrence counts.\n",
    "    \"\"\"\n",
    "    # Convert to GeoDataFrame\n",
    "    occ_gdf = gpd.GeoDataFrame(\n",
    "        occurrence_df,\n",
    "        geometry=gpd.points_from_xy(\n",
    "            occurrence_df['decimalLongitude'],\n",
    "            occurrence_df['decimalLatitude']\n",
    "        ),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Map to grid\n",
    "    occ_joined = map_points_to_grid(occ_gdf, grid_gdf)\n",
    "    \n",
    "    # Aggregate by grid × time × species\n",
    "    agg = occ_joined.groupby(['grid_id', time_unit, 'species']).size().reset_index(name='n_occurrences')\n",
    "    \n",
    "    # Add effort proxy (number of unique collection events)\n",
    "    if 'eventID' in occ_joined.columns:\n",
    "        effort = occ_joined.groupby(['grid_id', time_unit])['eventID'].nunique().reset_index(name='n_events')\n",
    "        agg = agg.merge(effort, on=['grid_id', time_unit], how='left')\n",
    "    \n",
    "    # Calculate detection-adjusted occupancy\n",
    "    agg['occupancy'] = (agg['n_occurrences'] > 0).astype(int)\n",
    "    \n",
    "    return agg\n",
    "\n",
    "# Example usage: Load GBIF data if available\n",
    "gbif_path = CONFIG['data_dir'] / 'raw' / 'gbif' / 'occurrences.csv'\n",
    "\n",
    "if gbif_path.exists():\n",
    "    print(f\"Found GBIF data at {gbif_path}\")\n",
    "    print(\"Loading and processing occurrence data...\")\n",
    "    \n",
    "    # Get species list from previous step\n",
    "    if 'species_df' in locals() and not species_df.empty:\n",
    "        species_names = species_df['scientificName'].tolist()\n",
    "        \n",
    "        # Load and process occurrences\n",
    "        occurrence_df = load_and_process_gbif_occurrences(\n",
    "            gbif_path,\n",
    "            species_names,\n",
    "            CONFIG['time_range'][0],\n",
    "            CONFIG['time_range'][1],\n",
    "            CONFIG['region_bbox']\n",
    "        )\n",
    "        \n",
    "        if occurrence_df is not None and len(occurrence_df) > 0:\n",
    "            # Build occurrence panel\n",
    "            if 'grid_gdf' in locals():\n",
    "                occurrence_panel = build_occurrence_panel(occurrence_df, grid_gdf, time_unit='year')\n",
    "                \n",
    "                print(f\"\\nOccurrence panel created:\")\n",
    "                print(f\"  Grid cells: {occurrence_panel['grid_id'].nunique():,}\")\n",
    "                print(f\"  Species: {occurrence_panel['species'].nunique()}\")\n",
    "                print(f\"  Years: {occurrence_panel['year'].min()} - {occurrence_panel['year'].max()}\")\n",
    "                print(f\"  Total records: {len(occurrence_panel):,}\")\n",
    "                \n",
    "                # Save panel\n",
    "                panel_path = CONFIG['data_dir'] / 'processed' / 'occurrence_panel.csv'\n",
    "                occurrence_panel.to_csv(panel_path, index=False)\n",
    "                print(f\"\\nSaved occurrence panel to {panel_path}\")\n",
    "                \n",
    "                # Display summary statistics\n",
    "                print(\"\\nOccurrence summary by species:\")\n",
    "                species_summary = occurrence_panel.groupby('species').agg({\n",
    "                    'n_occurrences': 'sum',\n",
    "                    'grid_id': 'nunique'\n",
    "                }).rename(columns={'grid_id': 'n_grid_cells'}).sort_values('n_occurrences', ascending=False)\n",
    "                print(species_summary.head(10))\n",
    "            else:\n",
    "                print(\"ERROR: Grid not found. Run cell 5 to create analysis grid first.\")\n",
    "        else:\n",
    "            print(\"No occurrence data loaded. Check GBIF file format.\")\n",
    "    else:\n",
    "        print(\"Species list not found. Run cell 4a to get species list first.\")\n",
    "else:\n",
    "    print(f\"GBIF data not found at {gbif_path}\")\n",
    "    print(\"\\nTo proceed:\")\n",
    "    print(\"1. Run cells 4a and 4b to request GBIF download\")\n",
    "    print(\"2. Wait for email notification from GBIF\")\n",
    "    print(\"3. Place downloaded occurrence file at:\")\n",
    "    print(f\"   {gbif_path}\")\n",
    "    print(\"4. Re-run this cell to process the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Treatment panel construction\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "def build_treatment_panel(disaster_df, grid_gdf, time_unit='year', buffer_km=50):\n",
    "    \"\"\"\n",
    "    Create treatment indicators for grid × time.\n",
    "    \n",
    "    Parameters:\n",
    "    - disaster_df: DataFrame with columns [year, latitude, longitude, disaster_type, ...]\n",
    "    - grid_gdf: Grid GeoDataFrame\n",
    "    - buffer_km: radius to expand point events (default 50km)\n",
    "    \n",
    "    Returns: DataFrame with [grid_id, year, treated, disaster_type, intensity]\n",
    "    \"\"\"\n",
    "    if disaster_df is None or len(disaster_df) == 0:\n",
    "        print(\"No disaster data provided\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    disaster_gdf = gpd.GeoDataFrame(\n",
    "        disaster_df,\n",
    "        geometry=gpd.points_from_xy(disaster_df['longitude'], disaster_df['latitude']),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    \n",
    "    # Expand to footprints\n",
    "    affected = expand_disaster_footprint(disaster_gdf, buffer_km, grid_gdf)\n",
    "    \n",
    "    # Aggregate by grid × time\n",
    "    treatment = affected.groupby(['grid_id', time_unit]).agg({\n",
    "        'disaster_type': 'first',  # Take first if multiple\n",
    "        'overlap_fraction': 'sum'  # Sum overlaps (can be >1 if multiple events)\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create binary treatment\n",
    "    treatment['treated'] = 1\n",
    "    treatment['treatment_intensity'] = treatment['overlap_fraction']\n",
    "    \n",
    "    # Create event timing for event study\n",
    "    # For each grid, find first treatment year\n",
    "    first_treatment = treatment.groupby('grid_id')[time_unit].min().reset_index()\n",
    "    first_treatment.columns = ['grid_id', 'first_treatment_year']\n",
    "    treatment = treatment.merge(first_treatment, on='grid_id', how='left')\n",
    "    \n",
    "    return treatment\n",
    "\n",
    "def create_full_panel(grid_gdf, years, treatment_df=None):\n",
    "    \"\"\"\n",
    "    Create complete grid × year panel with treatment indicators.\n",
    "    \"\"\"\n",
    "    # All grid × year combinations\n",
    "    years_list = list(range(years[0], years[1] + 1))\n",
    "    panel = pd.DataFrame([\n",
    "        {'grid_id': gid, 'year': yr}\n",
    "        for gid in grid_gdf['grid_id']\n",
    "        for yr in years_list\n",
    "    ])\n",
    "    \n",
    "    # Merge treatment\n",
    "    if treatment_df is not None:\n",
    "        panel = panel.merge(\n",
    "            treatment_df[['grid_id', 'year', 'treated', 'treatment_intensity', 'first_treatment_year']],\n",
    "            on=['grid_id', 'year'],\n",
    "            how='left'\n",
    "        )\n",
    "        panel['treated'] = panel['treated'].fillna(0)\n",
    "        panel['treatment_intensity'] = panel['treatment_intensity'].fillna(0)\n",
    "    else:\n",
    "        panel['treated'] = 0\n",
    "        panel['treatment_intensity'] = 0\n",
    "    \n",
    "    return panel\n",
    "\n",
    "print(\"Treatment construction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbee141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Difference-in-Differences estimation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linearmodels.panel import PanelOLS\n",
    "from linearmodels import IV2SLS\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "def prepare_panel_data(outcome_df, treatment_df, grid_gdf):\n",
    "    \"\"\"\n",
    "    Merge outcome and treatment into analysis panel.\n",
    "    \"\"\"\n",
    "    # Merge treatment with outcomes\n",
    "    panel = outcome_df.merge(\n",
    "        treatment_df[['grid_id', 'year', 'treated', 'treatment_intensity', 'first_treatment_year']],\n",
    "        on=['grid_id', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing treatment as control\n",
    "    panel['treated'] = panel['treated'].fillna(0)\n",
    "    panel['treatment_intensity'] = panel['treatment_intensity'].fillna(0)\n",
    "    \n",
    "    # Add grid characteristics\n",
    "    panel = panel.merge(\n",
    "        grid_gdf[['grid_id', 'lon_center', 'lat_center', 'area_km2']],\n",
    "        on='grid_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return panel\n",
    "\n",
    "def estimate_twfe_did(panel_df, outcome_var, treatment_var='treated', entity_var='grid_id', time_var='year'):\n",
    "    \"\"\"\n",
    "    Two-way fixed effects DiD using PanelOLS.\n",
    "    \"\"\"\n",
    "    # Set multi-index for panel\n",
    "    panel = panel_df.copy()\n",
    "    panel = panel.set_index([entity_var, time_var])\n",
    "    \n",
    "    # Estimate with entity and time fixed effects\n",
    "    formula = f'{outcome_var} ~ {treatment_var} + EntityEffects + TimeEffects'\n",
    "    \n",
    "    try:\n",
    "        mod = PanelOLS.from_formula(formula, data=panel)\n",
    "        res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "        \n",
    "        print(\"Two-Way Fixed Effects DiD Results\")\n",
    "        print(\"=\" * 60)\n",
    "        print(res.summary)\n",
    "        \n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in TWFE estimation: {e}\")\n",
    "        return None\n",
    "\n",
    "def estimate_event_study(panel_df, outcome_var, entity_var='grid_id', time_var='year', \n",
    "                        treatment_time_var='first_treatment_year', window=(-3, 5)):\n",
    "    \"\"\"\n",
    "    Event study with leads and lags.\n",
    "    \"\"\"\n",
    "    panel = panel_df.copy()\n",
    "    \n",
    "    # Calculate event time (years relative to treatment)\n",
    "    panel['event_time'] = panel[time_var] - panel[treatment_time_var]\n",
    "    \n",
    "    # Create event time dummies (omit -1 as reference)\n",
    "    for t in range(window[0], window[1] + 1):\n",
    "        if t == -1:\n",
    "            continue  # Reference period\n",
    "        panel[f'lead_lag_{t}'] = (panel['event_time'] == t).astype(int)\n",
    "    \n",
    "    # Only include treated units and within window\n",
    "    panel_es = panel[panel[treatment_time_var].notna()].copy()\n",
    "    panel_es = panel_es[panel_es['event_time'].between(window[0], window[1])]\n",
    "    \n",
    "    # Set index\n",
    "    panel_es = panel_es.set_index([entity_var, time_var])\n",
    "    \n",
    "    # Build formula\n",
    "    lead_lag_vars = [f'lead_lag_{t}' for t in range(window[0], window[1] + 1) if t != -1]\n",
    "    formula = f\"{outcome_var} ~ {' + '.join(lead_lag_vars)} + EntityEffects + TimeEffects\"\n",
    "    \n",
    "    try:\n",
    "        mod = PanelOLS.from_formula(formula, data=panel_es)\n",
    "        res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "        \n",
    "        print(\"Event Study Results\")\n",
    "        print(\"=\" * 60)\n",
    "        print(res.summary)\n",
    "        \n",
    "        # Extract coefficients for plotting\n",
    "        coefs = []\n",
    "        for t in range(window[0], window[1] + 1):\n",
    "            if t == -1:\n",
    "                coefs.append({'event_time': t, 'coef': 0, 'se': 0, 'ci_lower': 0, 'ci_upper': 0})\n",
    "            else:\n",
    "                var_name = f'lead_lag_{t}'\n",
    "                if var_name in res.params.index:\n",
    "                    coef = res.params[var_name]\n",
    "                    se = res.std_errors[var_name]\n",
    "                    ci_lower = coef - 1.96 * se\n",
    "                    ci_upper = coef + 1.96 * se\n",
    "                    coefs.append({'event_time': t, 'coef': coef, 'se': se, 'ci_lower': ci_lower, 'ci_upper': ci_upper})\n",
    "        \n",
    "        coefs_df = pd.DataFrame(coefs)\n",
    "        \n",
    "        return res, coefs_df\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in event study: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"DiD estimation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Visualization functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_event_study(coefs_df, outcome_name='Outcome', save_path=None):\n",
    "    \"\"\"\n",
    "    Plot event study coefficients with confidence intervals.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.fill_between(\n",
    "        coefs_df['event_time'],\n",
    "        coefs_df['ci_lower'],\n",
    "        coefs_df['ci_upper'],\n",
    "        alpha=0.2,\n",
    "        color='steelblue'\n",
    "    )\n",
    "    \n",
    "    plt.plot(\n",
    "        coefs_df['event_time'],\n",
    "        coefs_df['coef'],\n",
    "        marker='o',\n",
    "        color='steelblue',\n",
    "        linewidth=2,\n",
    "        markersize=6\n",
    "    )\n",
    "    \n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(-0.5, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('Years Relative to Disaster', fontsize=12)\n",
    "    plt.ylabel(f'Effect on {outcome_name}', fontsize=12)\n",
    "    plt.title('Event Study: Disaster Impact on Species Occurrences', fontsize=14, fontweight='bold')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_treatment_map(grid_gdf, treatment_df, year, save_path=None):\n",
    "    \"\"\"\n",
    "    Map treated grid cells for a specific year.\n",
    "    \"\"\"\n",
    "    # Filter treatment to year\n",
    "    treated_year = treatment_df[treatment_df['year'] == year].copy()\n",
    "    \n",
    "    # Merge with grid\n",
    "    grid_plot = grid_gdf.merge(treated_year[['grid_id', 'treated']], on='grid_id', how='left')\n",
    "    grid_plot['treated'] = grid_plot['treated'].fillna(0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    grid_plot.plot(\n",
    "        column='treated',\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        cmap='RdYlGn_r',\n",
    "        edgecolor='none',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Disaster-Affected Grid Cells in {year}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_species_effects(results_by_species, save_path=None):\n",
    "    \"\"\"\n",
    "    Forest plot of species-specific treatment effects.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, max(8, len(results_by_species) * 0.5)))\n",
    "    \n",
    "    species = list(results_by_species.keys())\n",
    "    coefs = [r['coef'] for r in results_by_species.values()]\n",
    "    ci_lower = [r['ci_lower'] for r in results_by_species.values()]\n",
    "    ci_upper = [r['ci_upper'] for r in results_by_species.values()]\n",
    "    \n",
    "    y_pos = range(len(species))\n",
    "    \n",
    "    plt.errorbar(coefs, y_pos, xerr=[\n",
    "        [c - ci_l for c, ci_l in zip(coefs, ci_lower)],\n",
    "        [ci_u - c for c, ci_u in zip(coefs, ci_upper)]\n",
    "    ], fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "    \n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.yticks(y_pos, species)\n",
    "    plt.xlabel('Treatment Effect (DiD Coefficient)', fontsize=12)\n",
    "    plt.ylabel('Species', fontsize=12)\n",
    "    plt.title('Species-Specific Disaster Effects', fontsize=14, fontweight='bold')\n",
    "    plt.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ade06",
   "metadata": {},
   "source": [
    "## 11. Advanced Estimators\n",
    "\n",
    "### Synthetic Control Method\n",
    "For case studies with strong single treated unit (e.g., major disaster in specific region):\n",
    "\n",
    "```python\n",
    "# Placeholder for Synthetic Control\n",
    "# Use synthdid or R's Synth package via rpy2\n",
    "```\n",
    "\n",
    "### Sun-Abraham (2021) Estimator\n",
    "For heterogeneous treatment effects with staggered adoption:\n",
    "\n",
    "```python\n",
    "# Use pydid or R's did package\n",
    "```\n",
    "\n",
    "### Bayesian Hierarchical Model\n",
    "For pooling across species with varying effects:\n",
    "\n",
    "```python\n",
    "import pymc as pm\n",
    "\n",
    "# Model structure:\n",
    "# outcome_{i,s,t} ~ Normal(mu_{i,s,t}, sigma)\n",
    "# mu_{i,s,t} = alpha_s + beta_s * treatment_{i,t} + grid_effects + time_effects\n",
    "# alpha_s ~ Normal(mu_alpha, sigma_alpha)  # species intercepts\n",
    "# beta_s ~ Normal(mu_beta, sigma_beta)     # species treatment effects\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6f71b",
   "metadata": {},
   "source": [
    "## 12. Robustness & Falsification Checks\n",
    "\n",
    "### Checklist:\n",
    "1. **Pre-trend tests:** Visual inspection of event study + formal test of pre-treatment coefficients\n",
    "2. **Placebo tests:**\n",
    "   - Randomize treatment assignment to different grid cells\n",
    "   - Randomize treatment timing\n",
    "3. **Negative controls:** Species/regions not expected to be affected\n",
    "4. **Sensitivity analysis:**\n",
    "   - Alternative grid resolutions (0.05°, 0.2°)\n",
    "   - Alternative time aggregations (quarterly)\n",
    "   - Different treatment windows (buffer sizes)\n",
    "   - Exclude border cells\n",
    "5. **Specification checks:**\n",
    "   - Control for time-varying covariates (land cover change, human pressure)\n",
    "   - Alternative outcome definitions\n",
    "6. **Sample restrictions:**\n",
    "   - High-quality observations only\n",
    "   - Species with sufficient data\n",
    "7. **Inference:**\n",
    "   - Wild cluster bootstrap\n",
    "   - Randomization inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ec19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Robustness check implementations\n",
    "import numpy as np\n",
    "\n",
    "def pretrend_test(event_study_coefs, window=(-3, -1)):\n",
    "    \"\"\"\n",
    "    Test for parallel trends: joint F-test on pre-treatment coefficients.\n",
    "    \"\"\"\n",
    "    pre_coefs = event_study_coefs[\n",
    "        event_study_coefs['event_time'].between(window[0], window[1])\n",
    "    ]\n",
    "    \n",
    "    # Joint test: are all pre-treatment coefs jointly zero?\n",
    "    # Use F-statistic\n",
    "    mean_coef = pre_coefs['coef'].mean()\n",
    "    se = pre_coefs['se'].mean()  # Simplified\n",
    "    \n",
    "    f_stat = (mean_coef / se) ** 2\n",
    "    \n",
    "    print(\"Pre-Trend Test\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Mean pre-treatment coefficient: {mean_coef:.4f}\")\n",
    "    print(f\"F-statistic (simplified): {f_stat:.4f}\")\n",
    "    print(\"\\nInterpretation: Small F-stat and coefs close to 0 support parallel trends\")\n",
    "    \n",
    "    return f_stat\n",
    "\n",
    "def placebo_test(panel_df, outcome_var, n_iterations=100, seed=42):\n",
    "    \"\"\"\n",
    "    Placebo test: randomly assign treatment and estimate effects.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    placebo_coefs = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Shuffle treatment assignment\n",
    "        panel_placebo = panel_df.copy()\n",
    "        panel_placebo['treated'] = np.random.permutation(panel_placebo['treated'].values)\n",
    "        \n",
    "        # Estimate\n",
    "        try:\n",
    "            panel_placebo_indexed = panel_placebo.set_index(['grid_id', 'year'])\n",
    "            mod = PanelOLS.from_formula(\n",
    "                f'{outcome_var} ~ treated + EntityEffects + TimeEffects',\n",
    "                data=panel_placebo_indexed\n",
    "            )\n",
    "            res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "            placebo_coefs.append(res.params['treated'])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    placebo_coefs = np.array(placebo_coefs)\n",
    "    \n",
    "    print(\"Placebo Test Results\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Mean placebo coefficient: {placebo_coefs.mean():.4f}\")\n",
    "    print(f\"Std dev: {placebo_coefs.std():.4f}\")\n",
    "    print(f\"95% CI: [{np.percentile(placebo_coefs, 2.5):.4f}, {np.percentile(placebo_coefs, 97.5):.4f}]\")\n",
    "    \n",
    "    return placebo_coefs\n",
    "\n",
    "def sensitivity_alternative_resolution(occurrence_df, treatment_df, resolutions=[0.05, 0.1, 0.2]):\n",
    "    \"\"\"\n",
    "    Re-run analysis at different spatial resolutions.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for res in resolutions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing at {res}° resolution...\")\n",
    "        \n",
    "        # Create grid at this resolution\n",
    "        grid = create_analysis_grid(CONFIG['region_bbox'], res)\n",
    "        \n",
    "        # Re-aggregate data\n",
    "        # ... (implementation similar to main analysis)\n",
    "        \n",
    "        results[res] = {'grid': grid}  # Store results\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Robustness check functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Save results and processed data\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def save_analysis_results(results_dict, output_dir):\n",
    "    \"\"\"\n",
    "    Save all analysis outputs for reproducibility.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save data panels\n",
    "    if 'panel_data' in results_dict:\n",
    "        results_dict['panel_data'].to_parquet(output_dir / 'analysis_panel.parquet')\n",
    "    \n",
    "    # Save estimation results\n",
    "    if 'did_results' in results_dict:\n",
    "        with open(output_dir / 'did_results.pkl', 'wb') as f:\n",
    "            pickle.dump(results_dict['did_results'], f)\n",
    "    \n",
    "    if 'event_study_coefs' in results_dict:\n",
    "        results_dict['event_study_coefs'].to_csv(output_dir / 'event_study_coefs.csv', index=False)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary = {\n",
    "        'n_grid_cells': len(results_dict.get('grid', [])),\n",
    "        'n_treated_cells': results_dict.get('n_treated', 0),\n",
    "        'n_species': results_dict.get('n_species', 0),\n",
    "        'time_range': CONFIG['time_range'],\n",
    "        'main_effect': float(results_dict.get('main_coef', 0)),\n",
    "        'main_se': float(results_dict.get('main_se', 0))\n",
    "    }\n",
    "    \n",
    "    with open(output_dir / 'summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_dir}\")\n",
    "\n",
    "# Example usage (uncomment when results are ready):\n",
    "# results = {\n",
    "#     'panel_data': panel_df,\n",
    "#     'did_results': did_res,\n",
    "#     'event_study_coefs': event_study_coefs,\n",
    "#     'grid': grid_gdf,\n",
    "#     'n_treated': treatment_df['grid_id'].nunique(),\n",
    "#     'n_species': occurrence_df['species'].nunique()\n",
    "# }\n",
    "# save_analysis_results(results, CONFIG['results_dir'])\n",
    "\n",
    "print(\"Result saving functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7382a",
   "metadata": {},
   "source": [
    "## 15. Complete Analysis Workflow\n",
    "\n",
    "### Step-by-step execution:\n",
    "\n",
    "1. **Data acquisition:**\n",
    "   - Download EM-DAT disasters → `data/raw/emdat_asia_2000_2024.csv`\n",
    "   - Request GBIF download → wait for notification → download to `data/raw/gbif/occurrences.csv`\n",
    "   - Download IUCN ranges → extract to `data/raw/iucn/`\n",
    "   - (Optional) Download MODIS land cover → `data/raw/modis/`\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   ```python\n",
    "   # Load data\n",
    "   emdat_df = load_emdat_data(...)\n",
    "   gbif_df = load_and_process_gbif_occurrences(...)\n",
    "   iucn_ranges = load_iucn_ranges(...)\n",
    "   \n",
    "   # Create spatial grid\n",
    "   grid_gdf = create_analysis_grid(...)\n",
    "   \n",
    "   # Build panels\n",
    "   treatment_df = build_treatment_panel(emdat_df, grid_gdf)\n",
    "   occurrence_panel = build_occurrence_panel(gbif_df, grid_gdf)\n",
    "   \n",
    "   # Merge into analysis panel\n",
    "   panel_df = prepare_panel_data(occurrence_panel, treatment_df, grid_gdf)\n",
    "   ```\n",
    "\n",
    "3. **Main estimation:**\n",
    "   ```python\n",
    "   # DiD\n",
    "   did_res = estimate_twfe_did(panel_df, outcome_var='occupancy')\n",
    "   \n",
    "   # Event study\n",
    "   es_res, es_coefs = estimate_event_study(panel_df, outcome_var='occupancy')\n",
    "   \n",
    "   # Plot\n",
    "   plot_event_study(es_coefs, save_path=CONFIG['results_dir'] / 'event_study.png')\n",
    "   ```\n",
    "\n",
    "4. **Species-specific analysis:**\n",
    "   ```python\n",
    "   species_results = {}\n",
    "   for species in panel_df['species'].unique():\n",
    "       panel_sp = panel_df[panel_df['species'] == species]\n",
    "       res = estimate_twfe_did(panel_sp, outcome_var='occupancy')\n",
    "       species_results[species] = {\n",
    "           'coef': res.params['treated'],\n",
    "           'se': res.std_errors['treated'],\n",
    "           'ci_lower': res.conf_int().loc['treated', 'lower'],\n",
    "           'ci_upper': res.conf_int().loc['treated', 'upper']\n",
    "       }\n",
    "   \n",
    "   plot_species_effects(species_results)\n",
    "   ```\n",
    "\n",
    "5. **Robustness:**\n",
    "   ```python\n",
    "   # Pre-trends\n",
    "   f_stat = pretrend_test(es_coefs)\n",
    "   \n",
    "   # Placebo\n",
    "   placebo_coefs = placebo_test(panel_df, 'occupancy', n_iterations=500)\n",
    "   \n",
    "   # Sensitivity\n",
    "   sensitivity_results = sensitivity_alternative_resolution(occurrence_panel, treatment_df)\n",
    "   ```\n",
    "\n",
    "6. **Save outputs:**\n",
    "   ```python\n",
    "   results = {\n",
    "       'panel_data': panel_df,\n",
    "       'did_results': did_res,\n",
    "       'event_study_coefs': es_coefs,\n",
    "       'species_results': species_results,\n",
    "       'grid': grid_gdf,\n",
    "       'n_treated': treatment_df['grid_id'].nunique(),\n",
    "       'n_species': panel_df['species'].nunique(),\n",
    "       'main_coef': did_res.params['treated'],\n",
    "       'main_se': did_res.std_errors['treated']\n",
    "   }\n",
    "   \n",
    "   save_analysis_results(results, CONFIG['results_dir'])\n",
    "   ```\n",
    "\n",
    "### Next Steps:\n",
    "- Fill in API credentials in Cell 3\n",
    "- Download required datasets (see Data Sources in Cell 4)\n",
    "- Execute cells sequentially\n",
    "- Adjust parameters based on data characteristics\n",
    "- Iterate with robustness checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea280286",
   "metadata": {},
   "source": [
    "## 16. References & Resources\n",
    "\n",
    "**Key Papers:**\n",
    "- Callaway & Sant'Anna (2021): Difference-in-Differences with multiple time periods. *Journal of Econometrics*.\n",
    "- Sun & Abraham (2021): Estimating dynamic treatment effects. *Journal of Econometrics*.\n",
    "- Abadie et al. (2010): Synthetic control methods. *JASA*.\n",
    "\n",
    "**Data Sources:**\n",
    "- **EM-DAT:** https://www.emdat.be/\n",
    "- **GBIF:** https://www.gbif.org/ (API: https://www.gbif.org/developer/occurrence)\n",
    "- **IUCN Red List:** https://www.iucnredlist.org/resources/spatial-data-download\n",
    "- **MODIS:** https://modis.gsfc.nasa.gov/\n",
    "- **Copernicus:** https://emergency.copernicus.eu/\n",
    "\n",
    "**Software:**\n",
    "- `linearmodels` (Python): Panel data models - https://bashtage.github.io/linearmodels/\n",
    "- `pydid` (Python): DiD estimators - https://github.com/py-econometrics/pydid\n",
    "- `fixest` (R): Fast fixed effects - https://lrberge.github.io/fixest/\n",
    "- `did` (R): Callaway-Sant'Anna - https://bcallaway11.github.io/did/\n",
    "\n",
    "**Analysis Guidelines:**\n",
    "- Roth et al. (2023): What's Trending in Difference-in-Differences? *JBES*.\n",
    "- Cunningham (2021): Causal Inference: The Mixtape. Yale Press.\n",
    "\n",
    "---\n",
    "\n",
    "**Contact & Issues:**\n",
    "For questions about this pipeline, open an issue or contact the maintainer.\n",
    "\n",
    "**Citation:**\n",
    "If you use this pipeline, please cite relevant data sources and methods papers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
